{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clopensesame/Nazzaro_DSPN_S24/blob/main/Exercise_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xdZ1WjBmtDf"
      },
      "source": [
        "# Exercise 12: Cross validation\n",
        "-----\n",
        "\n",
        "In this exercise, we'll practice implementing cross validation techniques, including leave-one-out and k-fold cross validation. We'll use the `PimaIndiansDiabetes2` practice dataset, which has medical data on a group of Pima Native American women, including whether or not they have diabetes. This dataset is part of the `mlbench` package. We'll be using each person's medical history to predict whether or not they have been diagnosed with diabetes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BubJSLTamtDg"
      },
      "source": [
        "# 1: Data (1 pts)\n",
        "---\n",
        "\n",
        "Load the `tidyverse`, `boot`, and `mlbench` packages (you may need to install `boot` and `mlbench`).\n",
        "\n",
        "Load the `PimaIndiansDiabetes2` dataset using the `data()` function. Drop the `insulin` column (it just has a lot of missing data) and then drop `NA`s from the rest of the dataset. Save your updated dataset to a new variable name. Finally, print the dimensions of your new dataset, and look at the first few lines of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QNuKGb6emtDg",
        "vscode": {
          "languageId": "r"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552d10a6-dcb1-4110-8c8c-f75a9c141047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
            "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.5\n",
            "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
            "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.4     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
            "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.1\n",
            "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
            "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
            "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
          ]
        }
      ],
      "source": [
        "install.packages(c(\"boot\",\"mlbench\"))\n",
        "library(tidyverse)\n",
        "library(boot)\n",
        "library(mlbench)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data(PimaIndiansDiabetes2)\n",
        "d <- PimaIndiansDiabetes2\n",
        "dat <- d %>%\n",
        "  select(!insulin) %>%\n",
        "  na.omit()\n",
        "\n",
        "dim(dat)\n",
        "head(dat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "pRgm3MOiKFXA",
        "outputId": "dac9596e-2ef0-4d92-d24b-7cc8fef4023b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>532</li><li>8</li></ol>\n"
            ],
            "text/markdown": "1. 532\n2. 8\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 532\n\\item 8\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 532   8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 8</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>pregnant</th><th scope=col>glucose</th><th scope=col>pressure</th><th scope=col>triceps</th><th scope=col>mass</th><th scope=col>pedigree</th><th scope=col>age</th><th scope=col>diabetes</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>6</td><td>148</td><td>72</td><td>35</td><td>33.6</td><td>0.627</td><td>50</td><td>pos</td></tr>\n",
              "\t<tr><th scope=row>2</th><td>1</td><td> 85</td><td>66</td><td>29</td><td>26.6</td><td>0.351</td><td>31</td><td>neg</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>1</td><td> 89</td><td>66</td><td>23</td><td>28.1</td><td>0.167</td><td>21</td><td>neg</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>0</td><td>137</td><td>40</td><td>35</td><td>43.1</td><td>2.288</td><td>33</td><td>pos</td></tr>\n",
              "\t<tr><th scope=row>7</th><td>3</td><td> 78</td><td>50</td><td>32</td><td>31.0</td><td>0.248</td><td>26</td><td>pos</td></tr>\n",
              "\t<tr><th scope=row>9</th><td>2</td><td>197</td><td>70</td><td>45</td><td>30.5</td><td>0.158</td><td>53</td><td>pos</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 6 × 8\n\n| <!--/--> | pregnant &lt;dbl&gt; | glucose &lt;dbl&gt; | pressure &lt;dbl&gt; | triceps &lt;dbl&gt; | mass &lt;dbl&gt; | pedigree &lt;dbl&gt; | age &lt;dbl&gt; | diabetes &lt;fct&gt; |\n|---|---|---|---|---|---|---|---|---|\n| 1 | 6 | 148 | 72 | 35 | 33.6 | 0.627 | 50 | pos |\n| 2 | 1 |  85 | 66 | 29 | 26.6 | 0.351 | 31 | neg |\n| 4 | 1 |  89 | 66 | 23 | 28.1 | 0.167 | 21 | neg |\n| 5 | 0 | 137 | 40 | 35 | 43.1 | 2.288 | 33 | pos |\n| 7 | 3 |  78 | 50 | 32 | 31.0 | 0.248 | 26 | pos |\n| 9 | 2 | 197 | 70 | 45 | 30.5 | 0.158 | 53 | pos |\n\n",
            "text/latex": "A data.frame: 6 × 8\n\\begin{tabular}{r|llllllll}\n  & pregnant & glucose & pressure & triceps & mass & pedigree & age & diabetes\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n\\hline\n\t1 & 6 & 148 & 72 & 35 & 33.6 & 0.627 & 50 & pos\\\\\n\t2 & 1 &  85 & 66 & 29 & 26.6 & 0.351 & 31 & neg\\\\\n\t4 & 1 &  89 & 66 & 23 & 28.1 & 0.167 & 21 & neg\\\\\n\t5 & 0 & 137 & 40 & 35 & 43.1 & 2.288 & 33 & pos\\\\\n\t7 & 3 &  78 & 50 & 32 & 31.0 & 0.248 & 26 & pos\\\\\n\t9 & 2 & 197 & 70 & 45 & 30.5 & 0.158 & 53 & pos\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  pregnant glucose pressure triceps mass pedigree age diabetes\n",
              "1 6        148     72       35      33.6 0.627    50  pos     \n",
              "2 1         85     66       29      26.6 0.351    31  neg     \n",
              "4 1         89     66       23      28.1 0.167    21  neg     \n",
              "5 0        137     40       35      43.1 2.288    33  pos     \n",
              "7 3         78     50       32      31.0 0.248    26  pos     \n",
              "9 2        197     70       45      30.5 0.158    53  pos     "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BRVQiWSmtDh"
      },
      "source": [
        "(Note that in medical contexts, `pedigree` refers to a system of measuring family history of a condition. So here, higher numbers mean greater family history of diabetes. You can read more about this dataset [here](https://rdrr.io/cran/mlbench/man/PimaIndiansDiabetes.html).)\n",
        "\n",
        "# 2. Leave-one-out Cross Validation (4 pts)\n",
        "\n",
        "In the tutorial, we learned how to fit leave-one-out cross validation using the `cv.glm` function from the `boot` package. But we can also do this manually using `predict()` like we have in the past.\n",
        "\n",
        "Let's predict `diabetes`, a dichotomous outcome, using all the other variables in our modified dataset.\n",
        "\n",
        "First, fit a logistic regression model using all of the observations except the very first one. Then use your fitted model to predict whether your holdout case is positive or negative for diabetes. Remember that logistic regression coefficients are in **log-odds**, meaning that if an output is positive, the probability of the outcome is greater than 50%; if the output is negative, the probability of the outcome is less than 50%.\n",
        "\n",
        "Compare your result to the actual response in row one above. Did your model correctly classify this observation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "RDyry5h_mtDi",
        "vscode": {
          "languageId": "r"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37f2b7b3-9434-4fd2-e5e2-bd8b2037eda9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 531 × 9</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>pregnant</th><th scope=col>glucose</th><th scope=col>pressure</th><th scope=col>triceps</th><th scope=col>mass</th><th scope=col>pedigree</th><th scope=col>age</th><th scope=col>diabetes</th><th scope=col>diabetes_dic</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>2</th><td> 1</td><td> 85</td><td> 66</td><td>29</td><td>26.6</td><td>0.351</td><td>31</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>4</th><td> 1</td><td> 89</td><td> 66</td><td>23</td><td>28.1</td><td>0.167</td><td>21</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>5</th><td> 0</td><td>137</td><td> 40</td><td>35</td><td>43.1</td><td>2.288</td><td>33</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>7</th><td> 3</td><td> 78</td><td> 50</td><td>32</td><td>31.0</td><td>0.248</td><td>26</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>9</th><td> 2</td><td>197</td><td> 70</td><td>45</td><td>30.5</td><td>0.158</td><td>53</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>14</th><td> 1</td><td>189</td><td> 60</td><td>23</td><td>30.1</td><td>0.398</td><td>59</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>15</th><td> 5</td><td>166</td><td> 72</td><td>19</td><td>25.8</td><td>0.587</td><td>51</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>17</th><td> 0</td><td>118</td><td> 84</td><td>47</td><td>45.8</td><td>0.551</td><td>31</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>19</th><td> 1</td><td>103</td><td> 30</td><td>38</td><td>43.3</td><td>0.183</td><td>33</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>20</th><td> 1</td><td>115</td><td> 70</td><td>30</td><td>34.6</td><td>0.529</td><td>32</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>21</th><td> 3</td><td>126</td><td> 88</td><td>41</td><td>39.3</td><td>0.704</td><td>27</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>24</th><td> 9</td><td>119</td><td> 80</td><td>35</td><td>29.0</td><td>0.263</td><td>29</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>25</th><td>11</td><td>143</td><td> 94</td><td>33</td><td>36.6</td><td>0.254</td><td>51</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>26</th><td>10</td><td>125</td><td> 70</td><td>26</td><td>31.1</td><td>0.205</td><td>41</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>28</th><td> 1</td><td> 97</td><td> 66</td><td>15</td><td>23.2</td><td>0.487</td><td>22</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>29</th><td>13</td><td>145</td><td> 82</td><td>19</td><td>22.2</td><td>0.245</td><td>57</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>31</th><td> 5</td><td>109</td><td> 75</td><td>26</td><td>36.0</td><td>0.546</td><td>60</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>32</th><td> 3</td><td>158</td><td> 76</td><td>36</td><td>31.6</td><td>0.851</td><td>28</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>33</th><td> 3</td><td> 88</td><td> 58</td><td>11</td><td>24.8</td><td>0.267</td><td>22</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>35</th><td>10</td><td>122</td><td> 78</td><td>31</td><td>27.6</td><td>0.512</td><td>45</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>36</th><td> 4</td><td>103</td><td> 60</td><td>33</td><td>24.0</td><td>0.966</td><td>33</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>38</th><td> 9</td><td>102</td><td> 76</td><td>37</td><td>32.9</td><td>0.665</td><td>46</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>39</th><td> 2</td><td> 90</td><td> 68</td><td>42</td><td>38.2</td><td>0.503</td><td>27</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>40</th><td> 4</td><td>111</td><td> 72</td><td>47</td><td>37.1</td><td>1.390</td><td>56</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>41</th><td> 3</td><td>180</td><td> 64</td><td>25</td><td>34.0</td><td>0.271</td><td>26</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>43</th><td> 7</td><td>106</td><td> 92</td><td>18</td><td>22.7</td><td>0.235</td><td>48</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>44</th><td> 9</td><td>171</td><td>110</td><td>24</td><td>45.4</td><td>0.721</td><td>54</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>46</th><td> 0</td><td>180</td><td> 66</td><td>39</td><td>42.0</td><td>1.893</td><td>25</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>48</th><td> 2</td><td> 71</td><td> 70</td><td>27</td><td>28.0</td><td>0.586</td><td>22</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>49</th><td> 7</td><td>103</td><td> 66</td><td>32</td><td>39.1</td><td>0.344</td><td>31</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>⋮</th><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><th scope=row>726</th><td> 4</td><td>112</td><td>78</td><td>40</td><td>39.4</td><td>0.236</td><td>38</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>727</th><td> 1</td><td>116</td><td>78</td><td>29</td><td>36.1</td><td>0.496</td><td>25</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>728</th><td> 0</td><td>141</td><td>84</td><td>26</td><td>32.4</td><td>0.433</td><td>22</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>731</th><td> 3</td><td>130</td><td>78</td><td>23</td><td>28.4</td><td>0.323</td><td>34</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>733</th><td> 2</td><td>174</td><td>88</td><td>37</td><td>44.5</td><td>0.646</td><td>24</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>734</th><td> 2</td><td>106</td><td>56</td><td>27</td><td>29.0</td><td>0.426</td><td>22</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>736</th><td> 4</td><td> 95</td><td>60</td><td>32</td><td>35.4</td><td>0.284</td><td>28</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>737</th><td> 0</td><td>126</td><td>86</td><td>27</td><td>27.4</td><td>0.515</td><td>21</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>738</th><td> 8</td><td> 65</td><td>72</td><td>23</td><td>32.0</td><td>0.600</td><td>42</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>739</th><td> 2</td><td> 99</td><td>60</td><td>17</td><td>36.6</td><td>0.453</td><td>21</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>741</th><td>11</td><td>120</td><td>80</td><td>37</td><td>42.3</td><td>0.785</td><td>48</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>742</th><td> 3</td><td>102</td><td>44</td><td>20</td><td>30.8</td><td>0.400</td><td>26</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>743</th><td> 1</td><td>109</td><td>58</td><td>18</td><td>28.5</td><td>0.219</td><td>22</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>745</th><td>13</td><td>153</td><td>88</td><td>37</td><td>40.6</td><td>1.174</td><td>39</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>746</th><td>12</td><td>100</td><td>84</td><td>33</td><td>30.0</td><td>0.488</td><td>46</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>747</th><td> 1</td><td>147</td><td>94</td><td>41</td><td>49.3</td><td>0.358</td><td>27</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>748</th><td> 1</td><td> 81</td><td>74</td><td>41</td><td>46.3</td><td>1.096</td><td>32</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>749</th><td> 3</td><td>187</td><td>70</td><td>22</td><td>36.4</td><td>0.408</td><td>36</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>752</th><td> 1</td><td>121</td><td>78</td><td>39</td><td>39.0</td><td>0.261</td><td>28</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>753</th><td> 3</td><td>108</td><td>62</td><td>24</td><td>26.0</td><td>0.223</td><td>25</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>754</th><td> 0</td><td>181</td><td>88</td><td>44</td><td>43.3</td><td>0.222</td><td>26</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>755</th><td> 8</td><td>154</td><td>78</td><td>32</td><td>32.4</td><td>0.443</td><td>45</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>756</th><td> 1</td><td>128</td><td>88</td><td>39</td><td>36.5</td><td>1.057</td><td>37</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>757</th><td> 7</td><td>137</td><td>90</td><td>41</td><td>32.0</td><td>0.391</td><td>39</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>761</th><td> 2</td><td> 88</td><td>58</td><td>26</td><td>28.4</td><td>0.766</td><td>22</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>762</th><td> 9</td><td>170</td><td>74</td><td>31</td><td>44.0</td><td>0.403</td><td>43</td><td>pos</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>764</th><td>10</td><td>101</td><td>76</td><td>48</td><td>32.9</td><td>0.171</td><td>63</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>765</th><td> 2</td><td>122</td><td>70</td><td>27</td><td>36.8</td><td>0.340</td><td>27</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>766</th><td> 5</td><td>121</td><td>72</td><td>23</td><td>26.2</td><td>0.245</td><td>30</td><td>neg</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>768</th><td> 1</td><td> 93</td><td>70</td><td>31</td><td>30.4</td><td>0.315</td><td>23</td><td>neg</td><td>0</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 531 × 9\n\n| <!--/--> | pregnant &lt;dbl&gt; | glucose &lt;dbl&gt; | pressure &lt;dbl&gt; | triceps &lt;dbl&gt; | mass &lt;dbl&gt; | pedigree &lt;dbl&gt; | age &lt;dbl&gt; | diabetes &lt;fct&gt; | diabetes_dic &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|---|\n| 2 |  1 |  85 |  66 | 29 | 26.6 | 0.351 | 31 | neg | 0 |\n| 4 |  1 |  89 |  66 | 23 | 28.1 | 0.167 | 21 | neg | 0 |\n| 5 |  0 | 137 |  40 | 35 | 43.1 | 2.288 | 33 | pos | 1 |\n| 7 |  3 |  78 |  50 | 32 | 31.0 | 0.248 | 26 | pos | 1 |\n| 9 |  2 | 197 |  70 | 45 | 30.5 | 0.158 | 53 | pos | 1 |\n| 14 |  1 | 189 |  60 | 23 | 30.1 | 0.398 | 59 | pos | 1 |\n| 15 |  5 | 166 |  72 | 19 | 25.8 | 0.587 | 51 | pos | 1 |\n| 17 |  0 | 118 |  84 | 47 | 45.8 | 0.551 | 31 | pos | 1 |\n| 19 |  1 | 103 |  30 | 38 | 43.3 | 0.183 | 33 | neg | 0 |\n| 20 |  1 | 115 |  70 | 30 | 34.6 | 0.529 | 32 | pos | 1 |\n| 21 |  3 | 126 |  88 | 41 | 39.3 | 0.704 | 27 | neg | 0 |\n| 24 |  9 | 119 |  80 | 35 | 29.0 | 0.263 | 29 | pos | 1 |\n| 25 | 11 | 143 |  94 | 33 | 36.6 | 0.254 | 51 | pos | 1 |\n| 26 | 10 | 125 |  70 | 26 | 31.1 | 0.205 | 41 | pos | 1 |\n| 28 |  1 |  97 |  66 | 15 | 23.2 | 0.487 | 22 | neg | 0 |\n| 29 | 13 | 145 |  82 | 19 | 22.2 | 0.245 | 57 | neg | 0 |\n| 31 |  5 | 109 |  75 | 26 | 36.0 | 0.546 | 60 | neg | 0 |\n| 32 |  3 | 158 |  76 | 36 | 31.6 | 0.851 | 28 | pos | 1 |\n| 33 |  3 |  88 |  58 | 11 | 24.8 | 0.267 | 22 | neg | 0 |\n| 35 | 10 | 122 |  78 | 31 | 27.6 | 0.512 | 45 | neg | 0 |\n| 36 |  4 | 103 |  60 | 33 | 24.0 | 0.966 | 33 | neg | 0 |\n| 38 |  9 | 102 |  76 | 37 | 32.9 | 0.665 | 46 | pos | 1 |\n| 39 |  2 |  90 |  68 | 42 | 38.2 | 0.503 | 27 | pos | 1 |\n| 40 |  4 | 111 |  72 | 47 | 37.1 | 1.390 | 56 | pos | 1 |\n| 41 |  3 | 180 |  64 | 25 | 34.0 | 0.271 | 26 | neg | 0 |\n| 43 |  7 | 106 |  92 | 18 | 22.7 | 0.235 | 48 | neg | 0 |\n| 44 |  9 | 171 | 110 | 24 | 45.4 | 0.721 | 54 | pos | 1 |\n| 46 |  0 | 180 |  66 | 39 | 42.0 | 1.893 | 25 | pos | 1 |\n| 48 |  2 |  71 |  70 | 27 | 28.0 | 0.586 | 22 | neg | 0 |\n| 49 |  7 | 103 |  66 | 32 | 39.1 | 0.344 | 31 | pos | 1 |\n| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n| 726 |  4 | 112 | 78 | 40 | 39.4 | 0.236 | 38 | neg | 0 |\n| 727 |  1 | 116 | 78 | 29 | 36.1 | 0.496 | 25 | neg | 0 |\n| 728 |  0 | 141 | 84 | 26 | 32.4 | 0.433 | 22 | neg | 0 |\n| 731 |  3 | 130 | 78 | 23 | 28.4 | 0.323 | 34 | pos | 1 |\n| 733 |  2 | 174 | 88 | 37 | 44.5 | 0.646 | 24 | pos | 1 |\n| 734 |  2 | 106 | 56 | 27 | 29.0 | 0.426 | 22 | neg | 0 |\n| 736 |  4 |  95 | 60 | 32 | 35.4 | 0.284 | 28 | neg | 0 |\n| 737 |  0 | 126 | 86 | 27 | 27.4 | 0.515 | 21 | neg | 0 |\n| 738 |  8 |  65 | 72 | 23 | 32.0 | 0.600 | 42 | neg | 0 |\n| 739 |  2 |  99 | 60 | 17 | 36.6 | 0.453 | 21 | neg | 0 |\n| 741 | 11 | 120 | 80 | 37 | 42.3 | 0.785 | 48 | pos | 1 |\n| 742 |  3 | 102 | 44 | 20 | 30.8 | 0.400 | 26 | neg | 0 |\n| 743 |  1 | 109 | 58 | 18 | 28.5 | 0.219 | 22 | neg | 0 |\n| 745 | 13 | 153 | 88 | 37 | 40.6 | 1.174 | 39 | neg | 0 |\n| 746 | 12 | 100 | 84 | 33 | 30.0 | 0.488 | 46 | neg | 0 |\n| 747 |  1 | 147 | 94 | 41 | 49.3 | 0.358 | 27 | pos | 1 |\n| 748 |  1 |  81 | 74 | 41 | 46.3 | 1.096 | 32 | neg | 0 |\n| 749 |  3 | 187 | 70 | 22 | 36.4 | 0.408 | 36 | pos | 1 |\n| 752 |  1 | 121 | 78 | 39 | 39.0 | 0.261 | 28 | neg | 0 |\n| 753 |  3 | 108 | 62 | 24 | 26.0 | 0.223 | 25 | neg | 0 |\n| 754 |  0 | 181 | 88 | 44 | 43.3 | 0.222 | 26 | pos | 1 |\n| 755 |  8 | 154 | 78 | 32 | 32.4 | 0.443 | 45 | pos | 1 |\n| 756 |  1 | 128 | 88 | 39 | 36.5 | 1.057 | 37 | pos | 1 |\n| 757 |  7 | 137 | 90 | 41 | 32.0 | 0.391 | 39 | neg | 0 |\n| 761 |  2 |  88 | 58 | 26 | 28.4 | 0.766 | 22 | neg | 0 |\n| 762 |  9 | 170 | 74 | 31 | 44.0 | 0.403 | 43 | pos | 1 |\n| 764 | 10 | 101 | 76 | 48 | 32.9 | 0.171 | 63 | neg | 0 |\n| 765 |  2 | 122 | 70 | 27 | 36.8 | 0.340 | 27 | neg | 0 |\n| 766 |  5 | 121 | 72 | 23 | 26.2 | 0.245 | 30 | neg | 0 |\n| 768 |  1 |  93 | 70 | 31 | 30.4 | 0.315 | 23 | neg | 0 |\n\n",
            "text/latex": "A data.frame: 531 × 9\n\\begin{tabular}{r|lllllllll}\n  & pregnant & glucose & pressure & triceps & mass & pedigree & age & diabetes & diabetes\\_dic\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct> & <dbl>\\\\\n\\hline\n\t2 &  1 &  85 &  66 & 29 & 26.6 & 0.351 & 31 & neg & 0\\\\\n\t4 &  1 &  89 &  66 & 23 & 28.1 & 0.167 & 21 & neg & 0\\\\\n\t5 &  0 & 137 &  40 & 35 & 43.1 & 2.288 & 33 & pos & 1\\\\\n\t7 &  3 &  78 &  50 & 32 & 31.0 & 0.248 & 26 & pos & 1\\\\\n\t9 &  2 & 197 &  70 & 45 & 30.5 & 0.158 & 53 & pos & 1\\\\\n\t14 &  1 & 189 &  60 & 23 & 30.1 & 0.398 & 59 & pos & 1\\\\\n\t15 &  5 & 166 &  72 & 19 & 25.8 & 0.587 & 51 & pos & 1\\\\\n\t17 &  0 & 118 &  84 & 47 & 45.8 & 0.551 & 31 & pos & 1\\\\\n\t19 &  1 & 103 &  30 & 38 & 43.3 & 0.183 & 33 & neg & 0\\\\\n\t20 &  1 & 115 &  70 & 30 & 34.6 & 0.529 & 32 & pos & 1\\\\\n\t21 &  3 & 126 &  88 & 41 & 39.3 & 0.704 & 27 & neg & 0\\\\\n\t24 &  9 & 119 &  80 & 35 & 29.0 & 0.263 & 29 & pos & 1\\\\\n\t25 & 11 & 143 &  94 & 33 & 36.6 & 0.254 & 51 & pos & 1\\\\\n\t26 & 10 & 125 &  70 & 26 & 31.1 & 0.205 & 41 & pos & 1\\\\\n\t28 &  1 &  97 &  66 & 15 & 23.2 & 0.487 & 22 & neg & 0\\\\\n\t29 & 13 & 145 &  82 & 19 & 22.2 & 0.245 & 57 & neg & 0\\\\\n\t31 &  5 & 109 &  75 & 26 & 36.0 & 0.546 & 60 & neg & 0\\\\\n\t32 &  3 & 158 &  76 & 36 & 31.6 & 0.851 & 28 & pos & 1\\\\\n\t33 &  3 &  88 &  58 & 11 & 24.8 & 0.267 & 22 & neg & 0\\\\\n\t35 & 10 & 122 &  78 & 31 & 27.6 & 0.512 & 45 & neg & 0\\\\\n\t36 &  4 & 103 &  60 & 33 & 24.0 & 0.966 & 33 & neg & 0\\\\\n\t38 &  9 & 102 &  76 & 37 & 32.9 & 0.665 & 46 & pos & 1\\\\\n\t39 &  2 &  90 &  68 & 42 & 38.2 & 0.503 & 27 & pos & 1\\\\\n\t40 &  4 & 111 &  72 & 47 & 37.1 & 1.390 & 56 & pos & 1\\\\\n\t41 &  3 & 180 &  64 & 25 & 34.0 & 0.271 & 26 & neg & 0\\\\\n\t43 &  7 & 106 &  92 & 18 & 22.7 & 0.235 & 48 & neg & 0\\\\\n\t44 &  9 & 171 & 110 & 24 & 45.4 & 0.721 & 54 & pos & 1\\\\\n\t46 &  0 & 180 &  66 & 39 & 42.0 & 1.893 & 25 & pos & 1\\\\\n\t48 &  2 &  71 &  70 & 27 & 28.0 & 0.586 & 22 & neg & 0\\\\\n\t49 &  7 & 103 &  66 & 32 & 39.1 & 0.344 & 31 & pos & 1\\\\\n\t⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n\t726 &  4 & 112 & 78 & 40 & 39.4 & 0.236 & 38 & neg & 0\\\\\n\t727 &  1 & 116 & 78 & 29 & 36.1 & 0.496 & 25 & neg & 0\\\\\n\t728 &  0 & 141 & 84 & 26 & 32.4 & 0.433 & 22 & neg & 0\\\\\n\t731 &  3 & 130 & 78 & 23 & 28.4 & 0.323 & 34 & pos & 1\\\\\n\t733 &  2 & 174 & 88 & 37 & 44.5 & 0.646 & 24 & pos & 1\\\\\n\t734 &  2 & 106 & 56 & 27 & 29.0 & 0.426 & 22 & neg & 0\\\\\n\t736 &  4 &  95 & 60 & 32 & 35.4 & 0.284 & 28 & neg & 0\\\\\n\t737 &  0 & 126 & 86 & 27 & 27.4 & 0.515 & 21 & neg & 0\\\\\n\t738 &  8 &  65 & 72 & 23 & 32.0 & 0.600 & 42 & neg & 0\\\\\n\t739 &  2 &  99 & 60 & 17 & 36.6 & 0.453 & 21 & neg & 0\\\\\n\t741 & 11 & 120 & 80 & 37 & 42.3 & 0.785 & 48 & pos & 1\\\\\n\t742 &  3 & 102 & 44 & 20 & 30.8 & 0.400 & 26 & neg & 0\\\\\n\t743 &  1 & 109 & 58 & 18 & 28.5 & 0.219 & 22 & neg & 0\\\\\n\t745 & 13 & 153 & 88 & 37 & 40.6 & 1.174 & 39 & neg & 0\\\\\n\t746 & 12 & 100 & 84 & 33 & 30.0 & 0.488 & 46 & neg & 0\\\\\n\t747 &  1 & 147 & 94 & 41 & 49.3 & 0.358 & 27 & pos & 1\\\\\n\t748 &  1 &  81 & 74 & 41 & 46.3 & 1.096 & 32 & neg & 0\\\\\n\t749 &  3 & 187 & 70 & 22 & 36.4 & 0.408 & 36 & pos & 1\\\\\n\t752 &  1 & 121 & 78 & 39 & 39.0 & 0.261 & 28 & neg & 0\\\\\n\t753 &  3 & 108 & 62 & 24 & 26.0 & 0.223 & 25 & neg & 0\\\\\n\t754 &  0 & 181 & 88 & 44 & 43.3 & 0.222 & 26 & pos & 1\\\\\n\t755 &  8 & 154 & 78 & 32 & 32.4 & 0.443 & 45 & pos & 1\\\\\n\t756 &  1 & 128 & 88 & 39 & 36.5 & 1.057 & 37 & pos & 1\\\\\n\t757 &  7 & 137 & 90 & 41 & 32.0 & 0.391 & 39 & neg & 0\\\\\n\t761 &  2 &  88 & 58 & 26 & 28.4 & 0.766 & 22 & neg & 0\\\\\n\t762 &  9 & 170 & 74 & 31 & 44.0 & 0.403 & 43 & pos & 1\\\\\n\t764 & 10 & 101 & 76 & 48 & 32.9 & 0.171 & 63 & neg & 0\\\\\n\t765 &  2 & 122 & 70 & 27 & 36.8 & 0.340 & 27 & neg & 0\\\\\n\t766 &  5 & 121 & 72 & 23 & 26.2 & 0.245 & 30 & neg & 0\\\\\n\t768 &  1 &  93 & 70 & 31 & 30.4 & 0.315 & 23 & neg & 0\\\\\n\\end{tabular}\n",
            "text/plain": [
              "    pregnant glucose pressure triceps mass pedigree age diabetes diabetes_dic\n",
              "2    1        85      66      29      26.6 0.351    31  neg      0           \n",
              "4    1        89      66      23      28.1 0.167    21  neg      0           \n",
              "5    0       137      40      35      43.1 2.288    33  pos      1           \n",
              "7    3        78      50      32      31.0 0.248    26  pos      1           \n",
              "9    2       197      70      45      30.5 0.158    53  pos      1           \n",
              "14   1       189      60      23      30.1 0.398    59  pos      1           \n",
              "15   5       166      72      19      25.8 0.587    51  pos      1           \n",
              "17   0       118      84      47      45.8 0.551    31  pos      1           \n",
              "19   1       103      30      38      43.3 0.183    33  neg      0           \n",
              "20   1       115      70      30      34.6 0.529    32  pos      1           \n",
              "21   3       126      88      41      39.3 0.704    27  neg      0           \n",
              "24   9       119      80      35      29.0 0.263    29  pos      1           \n",
              "25  11       143      94      33      36.6 0.254    51  pos      1           \n",
              "26  10       125      70      26      31.1 0.205    41  pos      1           \n",
              "28   1        97      66      15      23.2 0.487    22  neg      0           \n",
              "29  13       145      82      19      22.2 0.245    57  neg      0           \n",
              "31   5       109      75      26      36.0 0.546    60  neg      0           \n",
              "32   3       158      76      36      31.6 0.851    28  pos      1           \n",
              "33   3        88      58      11      24.8 0.267    22  neg      0           \n",
              "35  10       122      78      31      27.6 0.512    45  neg      0           \n",
              "36   4       103      60      33      24.0 0.966    33  neg      0           \n",
              "38   9       102      76      37      32.9 0.665    46  pos      1           \n",
              "39   2        90      68      42      38.2 0.503    27  pos      1           \n",
              "40   4       111      72      47      37.1 1.390    56  pos      1           \n",
              "41   3       180      64      25      34.0 0.271    26  neg      0           \n",
              "43   7       106      92      18      22.7 0.235    48  neg      0           \n",
              "44   9       171     110      24      45.4 0.721    54  pos      1           \n",
              "46   0       180      66      39      42.0 1.893    25  pos      1           \n",
              "48   2        71      70      27      28.0 0.586    22  neg      0           \n",
              "49   7       103      66      32      39.1 0.344    31  pos      1           \n",
              "⋮   ⋮        ⋮       ⋮        ⋮       ⋮    ⋮        ⋮   ⋮        ⋮           \n",
              "726  4       112     78       40      39.4 0.236    38  neg      0           \n",
              "727  1       116     78       29      36.1 0.496    25  neg      0           \n",
              "728  0       141     84       26      32.4 0.433    22  neg      0           \n",
              "731  3       130     78       23      28.4 0.323    34  pos      1           \n",
              "733  2       174     88       37      44.5 0.646    24  pos      1           \n",
              "734  2       106     56       27      29.0 0.426    22  neg      0           \n",
              "736  4        95     60       32      35.4 0.284    28  neg      0           \n",
              "737  0       126     86       27      27.4 0.515    21  neg      0           \n",
              "738  8        65     72       23      32.0 0.600    42  neg      0           \n",
              "739  2        99     60       17      36.6 0.453    21  neg      0           \n",
              "741 11       120     80       37      42.3 0.785    48  pos      1           \n",
              "742  3       102     44       20      30.8 0.400    26  neg      0           \n",
              "743  1       109     58       18      28.5 0.219    22  neg      0           \n",
              "745 13       153     88       37      40.6 1.174    39  neg      0           \n",
              "746 12       100     84       33      30.0 0.488    46  neg      0           \n",
              "747  1       147     94       41      49.3 0.358    27  pos      1           \n",
              "748  1        81     74       41      46.3 1.096    32  neg      0           \n",
              "749  3       187     70       22      36.4 0.408    36  pos      1           \n",
              "752  1       121     78       39      39.0 0.261    28  neg      0           \n",
              "753  3       108     62       24      26.0 0.223    25  neg      0           \n",
              "754  0       181     88       44      43.3 0.222    26  pos      1           \n",
              "755  8       154     78       32      32.4 0.443    45  pos      1           \n",
              "756  1       128     88       39      36.5 1.057    37  pos      1           \n",
              "757  7       137     90       41      32.0 0.391    39  neg      0           \n",
              "761  2        88     58       26      28.4 0.766    22  neg      0           \n",
              "762  9       170     74       31      44.0 0.403    43  pos      1           \n",
              "764 10       101     76       48      32.9 0.171    63  neg      0           \n",
              "765  2       122     70       27      36.8 0.340    27  neg      0           \n",
              "766  5       121     72       23      26.2 0.245    30  neg      0           \n",
              "768  1        93     70       31      30.4 0.315    23  neg      0           "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "glm(formula = diabetes_dic ~ pregnant + glucose + pressure + \n",
              "    triceps + mass + pedigree + age, data = dat_no_first)\n",
              "\n",
              "Coefficients:\n",
              "              Estimate Std. Error t value Pr(>|t|)    \n",
              "(Intercept) -1.0356375  0.1189484  -8.707  < 2e-16 ***\n",
              "pregnant     0.0202261  0.0065740   3.077 0.002203 ** \n",
              "glucose      0.0060164  0.0005833  10.314  < 2e-16 ***\n",
              "pressure    -0.0008687  0.0015203  -0.571 0.567991    \n",
              "triceps      0.0005701  0.0021030   0.271 0.786444    \n",
              "mass         0.0119056  0.0033206   3.585 0.000368 ***\n",
              "pedigree     0.1800867  0.0494968   3.638 0.000302 ***\n",
              "age          0.0041663  0.0021723   1.918 0.055661 .  \n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "\n",
              "(Dispersion parameter for gaussian family taken to be 0.1470284)\n",
              "\n",
              "    Null deviance: 117.665  on 530  degrees of freedom\n",
              "Residual deviance:  76.896  on 523  degrees of freedom\n",
              "AIC: 498.86\n",
              "\n",
              "Number of Fisher Scoring iterations: 2\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<strong>1:</strong> 0.654817342005822"
            ],
            "text/markdown": "**1:** 0.654817342005822",
            "text/latex": "\\textbf{1:} 0.654817342005822",
            "text/plain": [
              "        1 \n",
              "0.6548173 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "pos\n",
              "<details>\n",
              "\t<summary style=display:list-item;cursor:pointer>\n",
              "\t\t<strong>Levels</strong>:\n",
              "\t</summary>\n",
              "\t<style>\n",
              "\t.list-inline {list-style: none; margin:0; padding: 0}\n",
              "\t.list-inline>li {display: inline-block}\n",
              "\t.list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "\t</style>\n",
              "\t<ol class=list-inline><li>'neg'</li><li>'pos'</li></ol>\n",
              "</details>"
            ],
            "text/markdown": "pos\n**Levels**: 1. 'neg'\n2. 'pos'\n\n\n",
            "text/latex": "pos\n\\emph{Levels}: \\begin{enumerate*}\n\\item 'neg'\n\\item 'pos'\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] pos\n",
              "Levels: neg pos"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dat <- dat %>%\n",
        "  mutate(diabetes_dic = case_when(diabetes==\"neg\"~0, diabetes==\"pos\"~1))\n",
        "\n",
        "dat_no_first <- dat[-1,]\n",
        "dat_no_first\n",
        "\n",
        "#this is log-odds; adding family=\"binomial\" into glm we get [0,0.5)=neg and [0.5,1]=pos i think\n",
        "glm.fit = glm(diabetes_dic~pregnant+glucose+pressure+triceps+mass+pedigree+age, data=dat_no_first)\n",
        "summary(glm.fit)\n",
        "\n",
        "predicted_probs <- predict(glm.fit, newdata = dat[1,], type = \"response\")\n",
        "predicted_probs\n",
        "dat[1,]$diabetes\n",
        "#prediction=0.65 > 0 so prediction = pos\n",
        "#actual = pos\n",
        "#model did correctly classify\n",
        "\n",
        "\n",
        "#more predictions for obs 2-5\n",
        "# predicted_probs2 <- predict(glm.fit, newdata = dat[2:5,], type = \"response\")\n",
        "# predicted_probs2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIPpDdd0mtDi"
      },
      "source": [
        "So we just calculated a single iteration of LOOCV. We used 531 rows of our data to fit a model to predict the outcome of the last row.\n",
        "\n",
        "Below, use a `for` loop to iterate through the rest of your dataset doing the same thing. You will need to:\n",
        "* Create a data frame `results` with two columns: one named `actual` which holds the true classification for each observation, and one named `predicted`, which should be filled with `NA`s. This is where you'll store the output of your loop.\n",
        "* Create a loop that runs through each row of your data, pulls that observation out, trains your model on the remaining data, and then tests the fitted model on your test observation.\n",
        "* Store your model *predictions* (\"pos\" or \"neg\" -- not the log-odds) in the `predicted` column of your `results` dataframe\n",
        "\n",
        "After you run your loop, print the first few lines of `results`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5Z1ijm_PmtDj",
        "vscode": {
          "languageId": "r"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "cb3224f5-dd46-43fd-f9cc-b7351ba11c41"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 2</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>actual</th><th scope=col>predicted</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>pos</td><td>pos</td></tr>\n",
              "\t<tr><th scope=row>2</th><td>neg</td><td>neg</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>neg</td><td>neg</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>pos</td><td>pos</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>pos</td><td>neg</td></tr>\n",
              "\t<tr><th scope=row>6</th><td>pos</td><td>pos</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 6 × 2\n\n| <!--/--> | actual &lt;fct&gt; | predicted &lt;chr&gt; |\n|---|---|---|\n| 1 | pos | pos |\n| 2 | neg | neg |\n| 3 | neg | neg |\n| 4 | pos | pos |\n| 5 | pos | neg |\n| 6 | pos | pos |\n\n",
            "text/latex": "A data.frame: 6 × 2\n\\begin{tabular}{r|ll}\n  & actual & predicted\\\\\n  & <fct> & <chr>\\\\\n\\hline\n\t1 & pos & pos\\\\\n\t2 & neg & neg\\\\\n\t3 & neg & neg\\\\\n\t4 & pos & pos\\\\\n\t5 & pos & neg\\\\\n\t6 & pos & pos\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  actual predicted\n",
              "1 pos    pos      \n",
              "2 neg    neg      \n",
              "3 neg    neg      \n",
              "4 pos    pos      \n",
              "5 pos    neg      \n",
              "6 pos    pos      "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize `results` data frame\n",
        "results = data.frame(actual = dat$diabetes,\n",
        "                     predicted = NA)\n",
        "\n",
        "#sanity check\n",
        "#nrow(results)\n",
        "#nrow(dat)\n",
        "\n",
        "#for loop\n",
        "for (i in 1:nrow(dat)){ #don't forget to change this to your data set name\n",
        "    # separate individual observation `i` from the rest of your data\n",
        "    dat_no_i <- dat[-i,]\n",
        "\n",
        "    # train your model\n",
        "    glm.fit = glm(diabetes_dic~pregnant+glucose+pressure+triceps+mass+pedigree+age, data=dat_no_i)\n",
        "\n",
        "    # test model on hold out observation\n",
        "    predicted_probs <- predict(glm.fit, newdata = dat[i,], type = \"response\")\n",
        "\n",
        "    # classify model prediction as \"pos\" or \"neg\" and add to `results`\n",
        "    results$predicted[i] <- ifelse(predicted_probs>0, \"pos\", \"neg\")\n",
        "}\n",
        "head(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Jng_EEmtDk"
      },
      "source": [
        "Now, calculate the overall error of your model. What proportion of cases were incorrectly classified?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "SYhKS5HimtDk",
        "vscode": {
          "languageId": "r"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "543ee2e9-1ba5-45b3-adda-149a11fe7ea9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.575187969924812"
            ],
            "text/markdown": "0.575187969924812",
            "text/latex": "0.575187969924812",
            "text/plain": [
              "[1] 0.575188"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "incorrect_count <- sum(results$actual != results$predicted)\n",
        "incorrect_proportion <- incorrect_count/nrow(results)\n",
        "\n",
        "incorrect_proportion\n",
        "#proportion of incorrectly classified cases: 0.58"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1TwL8BimtDl"
      },
      "source": [
        "# 3. Compare to `cv.glm` (3 pts)\n",
        "\n",
        "Now, let's compare this result to the `cv.glm` function. Using the tutorial as a guide, use `cv.glm` to run LOOCV on the data, using the same model (i.e., still using all of the variables to predict diabetes diagnosis).\n",
        "\n",
        "Note that, because this is a `classification` problem and not a regression problem like in the tutorial, we need to adjust the `cost` argument of `cv.glm`. We can read more about this in the docs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "wIUAW1AtmtDl",
        "vscode": {
          "languageId": "r"
        }
      },
      "outputs": [],
      "source": [
        "glm.fit = glm(diabetes_dic~pregnant+glucose+pressure+triceps+mass+pedigree+age, data=dat)\n",
        "#don't need to specify r, pi i think\n",
        "cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)\n",
        "\n",
        "cv.err  = cv.glm(dat, glm.fit, cost, K=nrow(dat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWhbyxtymtDl"
      },
      "source": [
        "Here, we see `cost` is defined as:\n",
        "> \"A function of two vector arguments specifying the cost function for the cross-validation. The first argument to cost should correspond to the **observed responses** and the second argument should correspond to the **predicted or fitted responses** from the generalized linear model.\"\n",
        "\n",
        "In the example code (scroll to bottom of the docs), we see that the appropriate cost function for a binary classification is\n",
        "\n",
        "``\n",
        "cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)\n",
        "``\n",
        "\n",
        "Where `r` is the vector of observed responses (technically \"pos\" and \"neg\", but R treats these as 1 and 0 under the hood), and `pi` is the vector of *probabilities* (not log-odds) fit by the model. Thus, this boils down to our error: what proportion of observations were incorrectly classified. You will need to include this code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "bJiCmfqomtDl",
        "vscode": {
          "languageId": "r"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b19670e8-c564-4aee-f1b8-be820996b14a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.219924812030076"
            ],
            "text/markdown": "0.219924812030076",
            "text/latex": "0.219924812030076",
            "text/plain": [
              "[1] 0.2199248"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#delta[1] == raw mse, delta[2] == adjusted mse\n",
        "cv.err$delta[1]\n",
        "#proportion of incorrectly classified cases: 0.22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcEuGX3FmtDm"
      },
      "source": [
        "How do your results compare to your manual LOOCV above?\n",
        "\n",
        "> * Manual LOOCV: proportion of incorrectly classified cases was 0.58\n",
        "> * LOOCV: proportion of incorrectly classified cases was 0.22\n",
        "> * LOOCV performed much better than Manual LOOCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y2tL4NpmtDm"
      },
      "source": [
        "# 4. Adjusting K and Reflection (2 pts)\n",
        "\n",
        "Recall that LOOCV has some drawbacks. In particular, it has quite high *variance* which can lead to poor performance on new test data. We can reduce this variance by increasing K.\n",
        "\n",
        "Below, re-run your cross validation using `cv.glm` with `k` set to 3, 5, 10, and 15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "vZYq_NQlmtDm",
        "scrolled": true,
        "vscode": {
          "languageId": "r"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "4664b8f3-b56a-4257-9538-cd0eb8d3579d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.216165413533835"
            ],
            "text/markdown": "0.216165413533835",
            "text/latex": "0.216165413533835",
            "text/plain": [
              "[1] 0.2161654"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.219924812030075"
            ],
            "text/markdown": "0.219924812030075",
            "text/latex": "0.219924812030075",
            "text/plain": [
              "[1] 0.2199248"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.223684210526316"
            ],
            "text/markdown": "0.223684210526316",
            "text/latex": "0.223684210526316",
            "text/plain": [
              "[1] 0.2236842"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.227443609022556"
            ],
            "text/markdown": "0.227443609022556",
            "text/latex": "0.227443609022556",
            "text/plain": [
              "[1] 0.2274436"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "set.seed(1)\n",
        "glm.fit = glm(diabetes_dic~pregnant+glucose+pressure+triceps+mass+pedigree+age, data=dat)\n",
        "cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)\n",
        "\n",
        "# K = 3\n",
        "cv.err3  = cv.glm(dat, glm.fit, cost, K=3)\n",
        "\n",
        "# K = 5\n",
        "cv.err5  = cv.glm(dat, glm.fit, cost, K=5)\n",
        "\n",
        "# K = 10\n",
        "cv.err10  = cv.glm(dat, glm.fit, cost, K=10)\n",
        "\n",
        "# K = 15\n",
        "cv.err15  = cv.glm(dat, glm.fit, cost, K=15)\n",
        "\n",
        "#oh they're similar, is that fine?\n",
        "cv.err3$delta[1] #error=0.22\n",
        "cv.err5$delta[1] #error=0.22\n",
        "cv.err10$delta[1] #error=0.22\n",
        "cv.err15$delta[1] #error=0.23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI5Y1OqOmtDn"
      },
      "source": [
        "#### Reflection\n",
        "\n",
        "How do your errors compare to your LOOCV error above? How do they change as k increases?\n",
        "> * The K=3,5,10,15 errors were pretty similar, and seemed to increase as k increased.\n",
        "\n",
        "If you change the random seed above, you'll get slightly different errors. If you were to do the same with your LOOCV above , would you expect to get different results each time? Why or why not?\n",
        "> * I would expect to get the same results for LOOCV regardless of the seed, since LOOCV runs on all combinations of the dataset minus one observation. We get different results for k-fold cv since the training set is a random selection of (k-1)/k x nrow(dat) observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IADueyGXG4Os"
      },
      "source": [
        "\n",
        "**DUE:** 5pm March 25, 2024\n",
        "\n",
        "**IMPORTANT** Did you collaborate with anyone on this assignment? If so, list their names here.\n",
        "> *Someone's Name*\n",
        ">\n",
        ">\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "4.2.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}